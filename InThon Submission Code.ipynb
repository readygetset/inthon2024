{"cells":[{"cell_type":"markdown","metadata":{"id":"nSpyBik--bLw"},"source":["# InThon Submission Code"]},{"cell_type":"markdown","metadata":{"id":"44Og9skt-hdd"},"source":["## Imports\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eNnfFxl-diRP","outputId":"7a44e0d6-0ba8-410c-fdd2-148903018732"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting git+https://github.com/salaniz/pycocoevalcap\n","  Cloning https://github.com/salaniz/pycocoevalcap to /tmp/pip-req-build-29qvkppl\n","  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap /tmp/pip-req-build-29qvkppl\n","  Resolved https://github.com/salaniz/pycocoevalcap to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pycocoevalcap==1.2) (2.0.8)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (3.8.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (1.26.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (10.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.16.0)\n"]}],"source":["!pip install -q git+https://github.com/huggingface/peft.git transformers bitsandbytes datasets\n","!pip install git+https://github.com/salaniz/pycocoevalcap\n","\n","import torch\n","from PIL import Image\n","import requests\n","from transformers import Blip2Processor, Blip2ForConditionalGeneration, InstructBlipProcessor, InstructBlipForConditionalGeneration, BitsAndBytesConfig\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JKwRbavGdkTR","outputId":"104239eb-5f4d-4fb3-a2fd-03d80efbdb5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["사용 중인 디바이스: cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"사용 중인 디바이스: {device}\")"]},{"cell_type":"markdown","metadata":{"id":"cXiJ_7KO-2HU"},"source":["## Load Model & Processor\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":215,"referenced_widgets":["976979ba6bdc41be800b5807fe46b9f3","5731b978f49b4d529d45c7d4ec1c4037","e0fff08c5eb94cb5a6e14a883a97548c","eb35675874ec4563b595c117cba67968","f571337c48a54c3fa0b2fcbb6aef909f","f01d59ce97464ead8774915ece792b2c","314c838c841e44b6b0a5c5da93c8a2c9","31aeb5284e5a4ffa9478c8fdf350eefe","319391f887bd455d8e07d31c27b6d97c","0f14caabd7d6481f9100ccdb6455724e","2611ab9861b9491c9635d0cbac11d10b"]},"id":"7BFTUbVEeCsa","outputId":"d716b8d0-81f9-4960-a58d-8f5087a1b42e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n","`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"976979ba6bdc41be800b5807fe46b9f3"}},"metadata":{}}],"source":["bnb_config = BitsAndBytesConfig(\n","    # load_in_8bit=True,\n","    load_in_4bit=True,\n","    # bnb_4bit_use_double_quant=True,\n","    # bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-13b\")\n","model = InstructBlipForConditionalGeneration.from_pretrained(\n","    \"Salesforce/instructblip-vicuna-13b\",\n","    quantization_config=bnb_config,\n","    # device_map=\"auto\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLmB7BH6Id12","outputId":"e767f2ad-6527-431e-f768-73c85818ef17"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","vision_model\n","vision_model.embeddings\n","vision_model.embeddings.patch_embedding\n","vision_model.encoder\n","vision_model.encoder.layers\n","vision_model.encoder.layers.0\n","vision_model.encoder.layers.0.self_attn\n","vision_model.encoder.layers.0.self_attn.dropout\n","vision_model.encoder.layers.0.self_attn.qkv\n","vision_model.encoder.layers.0.self_attn.projection\n","vision_model.encoder.layers.0.layer_norm1\n","vision_model.encoder.layers.0.mlp\n","vision_model.encoder.layers.0.mlp.activation_fn\n","vision_model.encoder.layers.0.mlp.fc1\n","vision_model.encoder.layers.0.mlp.fc2\n","vision_model.encoder.layers.0.layer_norm2\n","vision_model.encoder.layers.1\n","vision_model.encoder.layers.1.self_attn\n","vision_model.encoder.layers.1.self_attn.dropout\n","vision_model.encoder.layers.1.self_attn.qkv\n","vision_model.encoder.layers.1.self_attn.projection\n","vision_model.encoder.layers.1.layer_norm1\n","vision_model.encoder.layers.1.mlp\n","vision_model.encoder.layers.1.mlp.activation_fn\n","vision_model.encoder.layers.1.mlp.fc1\n","vision_model.encoder.layers.1.mlp.fc2\n","vision_model.encoder.layers.1.layer_norm2\n","vision_model.encoder.layers.2\n","vision_model.encoder.layers.2.self_attn\n","vision_model.encoder.layers.2.self_attn.dropout\n","vision_model.encoder.layers.2.self_attn.qkv\n","vision_model.encoder.layers.2.self_attn.projection\n","vision_model.encoder.layers.2.layer_norm1\n","vision_model.encoder.layers.2.mlp\n","vision_model.encoder.layers.2.mlp.activation_fn\n","vision_model.encoder.layers.2.mlp.fc1\n","vision_model.encoder.layers.2.mlp.fc2\n","vision_model.encoder.layers.2.layer_norm2\n","vision_model.encoder.layers.3\n","vision_model.encoder.layers.3.self_attn\n","vision_model.encoder.layers.3.self_attn.dropout\n","vision_model.encoder.layers.3.self_attn.qkv\n","vision_model.encoder.layers.3.self_attn.projection\n","vision_model.encoder.layers.3.layer_norm1\n","vision_model.encoder.layers.3.mlp\n","vision_model.encoder.layers.3.mlp.activation_fn\n","vision_model.encoder.layers.3.mlp.fc1\n","vision_model.encoder.layers.3.mlp.fc2\n","vision_model.encoder.layers.3.layer_norm2\n","vision_model.encoder.layers.4\n","vision_model.encoder.layers.4.self_attn\n","vision_model.encoder.layers.4.self_attn.dropout\n","vision_model.encoder.layers.4.self_attn.qkv\n","vision_model.encoder.layers.4.self_attn.projection\n","vision_model.encoder.layers.4.layer_norm1\n","vision_model.encoder.layers.4.mlp\n","vision_model.encoder.layers.4.mlp.activation_fn\n","vision_model.encoder.layers.4.mlp.fc1\n","vision_model.encoder.layers.4.mlp.fc2\n","vision_model.encoder.layers.4.layer_norm2\n","vision_model.encoder.layers.5\n","vision_model.encoder.layers.5.self_attn\n","vision_model.encoder.layers.5.self_attn.dropout\n","vision_model.encoder.layers.5.self_attn.qkv\n","vision_model.encoder.layers.5.self_attn.projection\n","vision_model.encoder.layers.5.layer_norm1\n","vision_model.encoder.layers.5.mlp\n","vision_model.encoder.layers.5.mlp.activation_fn\n","vision_model.encoder.layers.5.mlp.fc1\n","vision_model.encoder.layers.5.mlp.fc2\n","vision_model.encoder.layers.5.layer_norm2\n","vision_model.encoder.layers.6\n","vision_model.encoder.layers.6.self_attn\n","vision_model.encoder.layers.6.self_attn.dropout\n","vision_model.encoder.layers.6.self_attn.qkv\n","vision_model.encoder.layers.6.self_attn.projection\n","vision_model.encoder.layers.6.layer_norm1\n","vision_model.encoder.layers.6.mlp\n","vision_model.encoder.layers.6.mlp.activation_fn\n","vision_model.encoder.layers.6.mlp.fc1\n","vision_model.encoder.layers.6.mlp.fc2\n","vision_model.encoder.layers.6.layer_norm2\n","vision_model.encoder.layers.7\n","vision_model.encoder.layers.7.self_attn\n","vision_model.encoder.layers.7.self_attn.dropout\n","vision_model.encoder.layers.7.self_attn.qkv\n","vision_model.encoder.layers.7.self_attn.projection\n","vision_model.encoder.layers.7.layer_norm1\n","vision_model.encoder.layers.7.mlp\n","vision_model.encoder.layers.7.mlp.activation_fn\n","vision_model.encoder.layers.7.mlp.fc1\n","vision_model.encoder.layers.7.mlp.fc2\n","vision_model.encoder.layers.7.layer_norm2\n","vision_model.encoder.layers.8\n","vision_model.encoder.layers.8.self_attn\n","vision_model.encoder.layers.8.self_attn.dropout\n","vision_model.encoder.layers.8.self_attn.qkv\n","vision_model.encoder.layers.8.self_attn.projection\n","vision_model.encoder.layers.8.layer_norm1\n","vision_model.encoder.layers.8.mlp\n","vision_model.encoder.layers.8.mlp.activation_fn\n","vision_model.encoder.layers.8.mlp.fc1\n","vision_model.encoder.layers.8.mlp.fc2\n","vision_model.encoder.layers.8.layer_norm2\n","vision_model.encoder.layers.9\n","vision_model.encoder.layers.9.self_attn\n","vision_model.encoder.layers.9.self_attn.dropout\n","vision_model.encoder.layers.9.self_attn.qkv\n","vision_model.encoder.layers.9.self_attn.projection\n","vision_model.encoder.layers.9.layer_norm1\n","vision_model.encoder.layers.9.mlp\n","vision_model.encoder.layers.9.mlp.activation_fn\n","vision_model.encoder.layers.9.mlp.fc1\n","vision_model.encoder.layers.9.mlp.fc2\n","vision_model.encoder.layers.9.layer_norm2\n","vision_model.encoder.layers.10\n","vision_model.encoder.layers.10.self_attn\n","vision_model.encoder.layers.10.self_attn.dropout\n","vision_model.encoder.layers.10.self_attn.qkv\n","vision_model.encoder.layers.10.self_attn.projection\n","vision_model.encoder.layers.10.layer_norm1\n","vision_model.encoder.layers.10.mlp\n","vision_model.encoder.layers.10.mlp.activation_fn\n","vision_model.encoder.layers.10.mlp.fc1\n","vision_model.encoder.layers.10.mlp.fc2\n","vision_model.encoder.layers.10.layer_norm2\n","vision_model.encoder.layers.11\n","vision_model.encoder.layers.11.self_attn\n","vision_model.encoder.layers.11.self_attn.dropout\n","vision_model.encoder.layers.11.self_attn.qkv\n","vision_model.encoder.layers.11.self_attn.projection\n","vision_model.encoder.layers.11.layer_norm1\n","vision_model.encoder.layers.11.mlp\n","vision_model.encoder.layers.11.mlp.activation_fn\n","vision_model.encoder.layers.11.mlp.fc1\n","vision_model.encoder.layers.11.mlp.fc2\n","vision_model.encoder.layers.11.layer_norm2\n","vision_model.encoder.layers.12\n","vision_model.encoder.layers.12.self_attn\n","vision_model.encoder.layers.12.self_attn.dropout\n","vision_model.encoder.layers.12.self_attn.qkv\n","vision_model.encoder.layers.12.self_attn.projection\n","vision_model.encoder.layers.12.layer_norm1\n","vision_model.encoder.layers.12.mlp\n","vision_model.encoder.layers.12.mlp.activation_fn\n","vision_model.encoder.layers.12.mlp.fc1\n","vision_model.encoder.layers.12.mlp.fc2\n","vision_model.encoder.layers.12.layer_norm2\n","vision_model.encoder.layers.13\n","vision_model.encoder.layers.13.self_attn\n","vision_model.encoder.layers.13.self_attn.dropout\n","vision_model.encoder.layers.13.self_attn.qkv\n","vision_model.encoder.layers.13.self_attn.projection\n","vision_model.encoder.layers.13.layer_norm1\n","vision_model.encoder.layers.13.mlp\n","vision_model.encoder.layers.13.mlp.activation_fn\n","vision_model.encoder.layers.13.mlp.fc1\n","vision_model.encoder.layers.13.mlp.fc2\n","vision_model.encoder.layers.13.layer_norm2\n","vision_model.encoder.layers.14\n","vision_model.encoder.layers.14.self_attn\n","vision_model.encoder.layers.14.self_attn.dropout\n","vision_model.encoder.layers.14.self_attn.qkv\n","vision_model.encoder.layers.14.self_attn.projection\n","vision_model.encoder.layers.14.layer_norm1\n","vision_model.encoder.layers.14.mlp\n","vision_model.encoder.layers.14.mlp.activation_fn\n","vision_model.encoder.layers.14.mlp.fc1\n","vision_model.encoder.layers.14.mlp.fc2\n","vision_model.encoder.layers.14.layer_norm2\n","vision_model.encoder.layers.15\n","vision_model.encoder.layers.15.self_attn\n","vision_model.encoder.layers.15.self_attn.dropout\n","vision_model.encoder.layers.15.self_attn.qkv\n","vision_model.encoder.layers.15.self_attn.projection\n","vision_model.encoder.layers.15.layer_norm1\n","vision_model.encoder.layers.15.mlp\n","vision_model.encoder.layers.15.mlp.activation_fn\n","vision_model.encoder.layers.15.mlp.fc1\n","vision_model.encoder.layers.15.mlp.fc2\n","vision_model.encoder.layers.15.layer_norm2\n","vision_model.encoder.layers.16\n","vision_model.encoder.layers.16.self_attn\n","vision_model.encoder.layers.16.self_attn.dropout\n","vision_model.encoder.layers.16.self_attn.qkv\n","vision_model.encoder.layers.16.self_attn.projection\n","vision_model.encoder.layers.16.layer_norm1\n","vision_model.encoder.layers.16.mlp\n","vision_model.encoder.layers.16.mlp.activation_fn\n","vision_model.encoder.layers.16.mlp.fc1\n","vision_model.encoder.layers.16.mlp.fc2\n","vision_model.encoder.layers.16.layer_norm2\n","vision_model.encoder.layers.17\n","vision_model.encoder.layers.17.self_attn\n","vision_model.encoder.layers.17.self_attn.dropout\n","vision_model.encoder.layers.17.self_attn.qkv\n","vision_model.encoder.layers.17.self_attn.projection\n","vision_model.encoder.layers.17.layer_norm1\n","vision_model.encoder.layers.17.mlp\n","vision_model.encoder.layers.17.mlp.activation_fn\n","vision_model.encoder.layers.17.mlp.fc1\n","vision_model.encoder.layers.17.mlp.fc2\n","vision_model.encoder.layers.17.layer_norm2\n","vision_model.encoder.layers.18\n","vision_model.encoder.layers.18.self_attn\n","vision_model.encoder.layers.18.self_attn.dropout\n","vision_model.encoder.layers.18.self_attn.qkv\n","vision_model.encoder.layers.18.self_attn.projection\n","vision_model.encoder.layers.18.layer_norm1\n","vision_model.encoder.layers.18.mlp\n","vision_model.encoder.layers.18.mlp.activation_fn\n","vision_model.encoder.layers.18.mlp.fc1\n","vision_model.encoder.layers.18.mlp.fc2\n","vision_model.encoder.layers.18.layer_norm2\n","vision_model.encoder.layers.19\n","vision_model.encoder.layers.19.self_attn\n","vision_model.encoder.layers.19.self_attn.dropout\n","vision_model.encoder.layers.19.self_attn.qkv\n","vision_model.encoder.layers.19.self_attn.projection\n","vision_model.encoder.layers.19.layer_norm1\n","vision_model.encoder.layers.19.mlp\n","vision_model.encoder.layers.19.mlp.activation_fn\n","vision_model.encoder.layers.19.mlp.fc1\n","vision_model.encoder.layers.19.mlp.fc2\n","vision_model.encoder.layers.19.layer_norm2\n","vision_model.encoder.layers.20\n","vision_model.encoder.layers.20.self_attn\n","vision_model.encoder.layers.20.self_attn.dropout\n","vision_model.encoder.layers.20.self_attn.qkv\n","vision_model.encoder.layers.20.self_attn.projection\n","vision_model.encoder.layers.20.layer_norm1\n","vision_model.encoder.layers.20.mlp\n","vision_model.encoder.layers.20.mlp.activation_fn\n","vision_model.encoder.layers.20.mlp.fc1\n","vision_model.encoder.layers.20.mlp.fc2\n","vision_model.encoder.layers.20.layer_norm2\n","vision_model.encoder.layers.21\n","vision_model.encoder.layers.21.self_attn\n","vision_model.encoder.layers.21.self_attn.dropout\n","vision_model.encoder.layers.21.self_attn.qkv\n","vision_model.encoder.layers.21.self_attn.projection\n","vision_model.encoder.layers.21.layer_norm1\n","vision_model.encoder.layers.21.mlp\n","vision_model.encoder.layers.21.mlp.activation_fn\n","vision_model.encoder.layers.21.mlp.fc1\n","vision_model.encoder.layers.21.mlp.fc2\n","vision_model.encoder.layers.21.layer_norm2\n","vision_model.encoder.layers.22\n","vision_model.encoder.layers.22.self_attn\n","vision_model.encoder.layers.22.self_attn.dropout\n","vision_model.encoder.layers.22.self_attn.qkv\n","vision_model.encoder.layers.22.self_attn.projection\n","vision_model.encoder.layers.22.layer_norm1\n","vision_model.encoder.layers.22.mlp\n","vision_model.encoder.layers.22.mlp.activation_fn\n","vision_model.encoder.layers.22.mlp.fc1\n","vision_model.encoder.layers.22.mlp.fc2\n","vision_model.encoder.layers.22.layer_norm2\n","vision_model.encoder.layers.23\n","vision_model.encoder.layers.23.self_attn\n","vision_model.encoder.layers.23.self_attn.dropout\n","vision_model.encoder.layers.23.self_attn.qkv\n","vision_model.encoder.layers.23.self_attn.projection\n","vision_model.encoder.layers.23.layer_norm1\n","vision_model.encoder.layers.23.mlp\n","vision_model.encoder.layers.23.mlp.activation_fn\n","vision_model.encoder.layers.23.mlp.fc1\n","vision_model.encoder.layers.23.mlp.fc2\n","vision_model.encoder.layers.23.layer_norm2\n","vision_model.encoder.layers.24\n","vision_model.encoder.layers.24.self_attn\n","vision_model.encoder.layers.24.self_attn.dropout\n","vision_model.encoder.layers.24.self_attn.qkv\n","vision_model.encoder.layers.24.self_attn.projection\n","vision_model.encoder.layers.24.layer_norm1\n","vision_model.encoder.layers.24.mlp\n","vision_model.encoder.layers.24.mlp.activation_fn\n","vision_model.encoder.layers.24.mlp.fc1\n","vision_model.encoder.layers.24.mlp.fc2\n","vision_model.encoder.layers.24.layer_norm2\n","vision_model.encoder.layers.25\n","vision_model.encoder.layers.25.self_attn\n","vision_model.encoder.layers.25.self_attn.dropout\n","vision_model.encoder.layers.25.self_attn.qkv\n","vision_model.encoder.layers.25.self_attn.projection\n","vision_model.encoder.layers.25.layer_norm1\n","vision_model.encoder.layers.25.mlp\n","vision_model.encoder.layers.25.mlp.activation_fn\n","vision_model.encoder.layers.25.mlp.fc1\n","vision_model.encoder.layers.25.mlp.fc2\n","vision_model.encoder.layers.25.layer_norm2\n","vision_model.encoder.layers.26\n","vision_model.encoder.layers.26.self_attn\n","vision_model.encoder.layers.26.self_attn.dropout\n","vision_model.encoder.layers.26.self_attn.qkv\n","vision_model.encoder.layers.26.self_attn.projection\n","vision_model.encoder.layers.26.layer_norm1\n","vision_model.encoder.layers.26.mlp\n","vision_model.encoder.layers.26.mlp.activation_fn\n","vision_model.encoder.layers.26.mlp.fc1\n","vision_model.encoder.layers.26.mlp.fc2\n","vision_model.encoder.layers.26.layer_norm2\n","vision_model.encoder.layers.27\n","vision_model.encoder.layers.27.self_attn\n","vision_model.encoder.layers.27.self_attn.dropout\n","vision_model.encoder.layers.27.self_attn.qkv\n","vision_model.encoder.layers.27.self_attn.projection\n","vision_model.encoder.layers.27.layer_norm1\n","vision_model.encoder.layers.27.mlp\n","vision_model.encoder.layers.27.mlp.activation_fn\n","vision_model.encoder.layers.27.mlp.fc1\n","vision_model.encoder.layers.27.mlp.fc2\n","vision_model.encoder.layers.27.layer_norm2\n","vision_model.encoder.layers.28\n","vision_model.encoder.layers.28.self_attn\n","vision_model.encoder.layers.28.self_attn.dropout\n","vision_model.encoder.layers.28.self_attn.qkv\n","vision_model.encoder.layers.28.self_attn.projection\n","vision_model.encoder.layers.28.layer_norm1\n","vision_model.encoder.layers.28.mlp\n","vision_model.encoder.layers.28.mlp.activation_fn\n","vision_model.encoder.layers.28.mlp.fc1\n","vision_model.encoder.layers.28.mlp.fc2\n","vision_model.encoder.layers.28.layer_norm2\n","vision_model.encoder.layers.29\n","vision_model.encoder.layers.29.self_attn\n","vision_model.encoder.layers.29.self_attn.dropout\n","vision_model.encoder.layers.29.self_attn.qkv\n","vision_model.encoder.layers.29.self_attn.projection\n","vision_model.encoder.layers.29.layer_norm1\n","vision_model.encoder.layers.29.mlp\n","vision_model.encoder.layers.29.mlp.activation_fn\n","vision_model.encoder.layers.29.mlp.fc1\n","vision_model.encoder.layers.29.mlp.fc2\n","vision_model.encoder.layers.29.layer_norm2\n","vision_model.encoder.layers.30\n","vision_model.encoder.layers.30.self_attn\n","vision_model.encoder.layers.30.self_attn.dropout\n","vision_model.encoder.layers.30.self_attn.qkv\n","vision_model.encoder.layers.30.self_attn.projection\n","vision_model.encoder.layers.30.layer_norm1\n","vision_model.encoder.layers.30.mlp\n","vision_model.encoder.layers.30.mlp.activation_fn\n","vision_model.encoder.layers.30.mlp.fc1\n","vision_model.encoder.layers.30.mlp.fc2\n","vision_model.encoder.layers.30.layer_norm2\n","vision_model.encoder.layers.31\n","vision_model.encoder.layers.31.self_attn\n","vision_model.encoder.layers.31.self_attn.dropout\n","vision_model.encoder.layers.31.self_attn.qkv\n","vision_model.encoder.layers.31.self_attn.projection\n","vision_model.encoder.layers.31.layer_norm1\n","vision_model.encoder.layers.31.mlp\n","vision_model.encoder.layers.31.mlp.activation_fn\n","vision_model.encoder.layers.31.mlp.fc1\n","vision_model.encoder.layers.31.mlp.fc2\n","vision_model.encoder.layers.31.layer_norm2\n","vision_model.encoder.layers.32\n","vision_model.encoder.layers.32.self_attn\n","vision_model.encoder.layers.32.self_attn.dropout\n","vision_model.encoder.layers.32.self_attn.qkv\n","vision_model.encoder.layers.32.self_attn.projection\n","vision_model.encoder.layers.32.layer_norm1\n","vision_model.encoder.layers.32.mlp\n","vision_model.encoder.layers.32.mlp.activation_fn\n","vision_model.encoder.layers.32.mlp.fc1\n","vision_model.encoder.layers.32.mlp.fc2\n","vision_model.encoder.layers.32.layer_norm2\n","vision_model.encoder.layers.33\n","vision_model.encoder.layers.33.self_attn\n","vision_model.encoder.layers.33.self_attn.dropout\n","vision_model.encoder.layers.33.self_attn.qkv\n","vision_model.encoder.layers.33.self_attn.projection\n","vision_model.encoder.layers.33.layer_norm1\n","vision_model.encoder.layers.33.mlp\n","vision_model.encoder.layers.33.mlp.activation_fn\n","vision_model.encoder.layers.33.mlp.fc1\n","vision_model.encoder.layers.33.mlp.fc2\n","vision_model.encoder.layers.33.layer_norm2\n","vision_model.encoder.layers.34\n","vision_model.encoder.layers.34.self_attn\n","vision_model.encoder.layers.34.self_attn.dropout\n","vision_model.encoder.layers.34.self_attn.qkv\n","vision_model.encoder.layers.34.self_attn.projection\n","vision_model.encoder.layers.34.layer_norm1\n","vision_model.encoder.layers.34.mlp\n","vision_model.encoder.layers.34.mlp.activation_fn\n","vision_model.encoder.layers.34.mlp.fc1\n","vision_model.encoder.layers.34.mlp.fc2\n","vision_model.encoder.layers.34.layer_norm2\n","vision_model.encoder.layers.35\n","vision_model.encoder.layers.35.self_attn\n","vision_model.encoder.layers.35.self_attn.dropout\n","vision_model.encoder.layers.35.self_attn.qkv\n","vision_model.encoder.layers.35.self_attn.projection\n","vision_model.encoder.layers.35.layer_norm1\n","vision_model.encoder.layers.35.mlp\n","vision_model.encoder.layers.35.mlp.activation_fn\n","vision_model.encoder.layers.35.mlp.fc1\n","vision_model.encoder.layers.35.mlp.fc2\n","vision_model.encoder.layers.35.layer_norm2\n","vision_model.encoder.layers.36\n","vision_model.encoder.layers.36.self_attn\n","vision_model.encoder.layers.36.self_attn.dropout\n","vision_model.encoder.layers.36.self_attn.qkv\n","vision_model.encoder.layers.36.self_attn.projection\n","vision_model.encoder.layers.36.layer_norm1\n","vision_model.encoder.layers.36.mlp\n","vision_model.encoder.layers.36.mlp.activation_fn\n","vision_model.encoder.layers.36.mlp.fc1\n","vision_model.encoder.layers.36.mlp.fc2\n","vision_model.encoder.layers.36.layer_norm2\n","vision_model.encoder.layers.37\n","vision_model.encoder.layers.37.self_attn\n","vision_model.encoder.layers.37.self_attn.dropout\n","vision_model.encoder.layers.37.self_attn.qkv\n","vision_model.encoder.layers.37.self_attn.projection\n","vision_model.encoder.layers.37.layer_norm1\n","vision_model.encoder.layers.37.mlp\n","vision_model.encoder.layers.37.mlp.activation_fn\n","vision_model.encoder.layers.37.mlp.fc1\n","vision_model.encoder.layers.37.mlp.fc2\n","vision_model.encoder.layers.37.layer_norm2\n","vision_model.encoder.layers.38\n","vision_model.encoder.layers.38.self_attn\n","vision_model.encoder.layers.38.self_attn.dropout\n","vision_model.encoder.layers.38.self_attn.qkv\n","vision_model.encoder.layers.38.self_attn.projection\n","vision_model.encoder.layers.38.layer_norm1\n","vision_model.encoder.layers.38.mlp\n","vision_model.encoder.layers.38.mlp.activation_fn\n","vision_model.encoder.layers.38.mlp.fc1\n","vision_model.encoder.layers.38.mlp.fc2\n","vision_model.encoder.layers.38.layer_norm2\n","vision_model.post_layernorm\n","qformer\n","qformer.embeddings\n","qformer.embeddings.word_embeddings\n","qformer.embeddings.position_embeddings\n","qformer.embeddings.layernorm\n","qformer.embeddings.dropout\n","qformer.encoder\n","qformer.encoder.layer\n","qformer.encoder.layer.0\n","qformer.encoder.layer.0.attention\n","qformer.encoder.layer.0.attention.attention\n","qformer.encoder.layer.0.attention.attention.query\n","qformer.encoder.layer.0.attention.attention.key\n","qformer.encoder.layer.0.attention.attention.value\n","qformer.encoder.layer.0.attention.attention.dropout\n","qformer.encoder.layer.0.attention.output\n","qformer.encoder.layer.0.attention.output.dense\n","qformer.encoder.layer.0.attention.output.LayerNorm\n","qformer.encoder.layer.0.attention.output.dropout\n","qformer.encoder.layer.0.crossattention\n","qformer.encoder.layer.0.crossattention.attention\n","qformer.encoder.layer.0.crossattention.attention.query\n","qformer.encoder.layer.0.crossattention.attention.key\n","qformer.encoder.layer.0.crossattention.attention.value\n","qformer.encoder.layer.0.crossattention.attention.dropout\n","qformer.encoder.layer.0.crossattention.output\n","qformer.encoder.layer.0.crossattention.output.dense\n","qformer.encoder.layer.0.crossattention.output.LayerNorm\n","qformer.encoder.layer.0.crossattention.output.dropout\n","qformer.encoder.layer.0.intermediate\n","qformer.encoder.layer.0.intermediate.dense\n","qformer.encoder.layer.0.intermediate.intermediate_act_fn\n","qformer.encoder.layer.0.output\n","qformer.encoder.layer.0.output.dense\n","qformer.encoder.layer.0.output.LayerNorm\n","qformer.encoder.layer.0.output.dropout\n","qformer.encoder.layer.0.intermediate_query\n","qformer.encoder.layer.0.intermediate_query.dense\n","qformer.encoder.layer.0.intermediate_query.intermediate_act_fn\n","qformer.encoder.layer.0.output_query\n","qformer.encoder.layer.0.output_query.dense\n","qformer.encoder.layer.0.output_query.LayerNorm\n","qformer.encoder.layer.0.output_query.dropout\n","qformer.encoder.layer.1\n","qformer.encoder.layer.1.attention\n","qformer.encoder.layer.1.attention.attention\n","qformer.encoder.layer.1.attention.attention.query\n","qformer.encoder.layer.1.attention.attention.key\n","qformer.encoder.layer.1.attention.attention.value\n","qformer.encoder.layer.1.attention.attention.dropout\n","qformer.encoder.layer.1.attention.output\n","qformer.encoder.layer.1.attention.output.dense\n","qformer.encoder.layer.1.attention.output.LayerNorm\n","qformer.encoder.layer.1.attention.output.dropout\n","qformer.encoder.layer.1.intermediate\n","qformer.encoder.layer.1.intermediate.dense\n","qformer.encoder.layer.1.intermediate.intermediate_act_fn\n","qformer.encoder.layer.1.output\n","qformer.encoder.layer.1.output.dense\n","qformer.encoder.layer.1.output.LayerNorm\n","qformer.encoder.layer.1.output.dropout\n","qformer.encoder.layer.1.intermediate_query\n","qformer.encoder.layer.1.intermediate_query.dense\n","qformer.encoder.layer.1.intermediate_query.intermediate_act_fn\n","qformer.encoder.layer.1.output_query\n","qformer.encoder.layer.1.output_query.dense\n","qformer.encoder.layer.1.output_query.LayerNorm\n","qformer.encoder.layer.1.output_query.dropout\n","qformer.encoder.layer.2\n","qformer.encoder.layer.2.attention\n","qformer.encoder.layer.2.attention.attention\n","qformer.encoder.layer.2.attention.attention.query\n","qformer.encoder.layer.2.attention.attention.key\n","qformer.encoder.layer.2.attention.attention.value\n","qformer.encoder.layer.2.attention.attention.dropout\n","qformer.encoder.layer.2.attention.output\n","qformer.encoder.layer.2.attention.output.dense\n","qformer.encoder.layer.2.attention.output.LayerNorm\n","qformer.encoder.layer.2.attention.output.dropout\n","qformer.encoder.layer.2.crossattention\n","qformer.encoder.layer.2.crossattention.attention\n","qformer.encoder.layer.2.crossattention.attention.query\n","qformer.encoder.layer.2.crossattention.attention.key\n","qformer.encoder.layer.2.crossattention.attention.value\n","qformer.encoder.layer.2.crossattention.attention.dropout\n","qformer.encoder.layer.2.crossattention.output\n","qformer.encoder.layer.2.crossattention.output.dense\n","qformer.encoder.layer.2.crossattention.output.LayerNorm\n","qformer.encoder.layer.2.crossattention.output.dropout\n","qformer.encoder.layer.2.intermediate\n","qformer.encoder.layer.2.intermediate.dense\n","qformer.encoder.layer.2.intermediate.intermediate_act_fn\n","qformer.encoder.layer.2.output\n","qformer.encoder.layer.2.output.dense\n","qformer.encoder.layer.2.output.LayerNorm\n","qformer.encoder.layer.2.output.dropout\n","qformer.encoder.layer.2.intermediate_query\n","qformer.encoder.layer.2.intermediate_query.dense\n","qformer.encoder.layer.2.intermediate_query.intermediate_act_fn\n","qformer.encoder.layer.2.output_query\n","qformer.encoder.layer.2.output_query.dense\n","qformer.encoder.layer.2.output_query.LayerNorm\n","qformer.encoder.layer.2.output_query.dropout\n","qformer.encoder.layer.3\n","qformer.encoder.layer.3.attention\n","qformer.encoder.layer.3.attention.attention\n","qformer.encoder.layer.3.attention.attention.query\n","qformer.encoder.layer.3.attention.attention.key\n","qformer.encoder.layer.3.attention.attention.value\n","qformer.encoder.layer.3.attention.attention.dropout\n","qformer.encoder.layer.3.attention.output\n","qformer.encoder.layer.3.attention.output.dense\n","qformer.encoder.layer.3.attention.output.LayerNorm\n","qformer.encoder.layer.3.attention.output.dropout\n","qformer.encoder.layer.3.intermediate\n","qformer.encoder.layer.3.intermediate.dense\n","qformer.encoder.layer.3.intermediate.intermediate_act_fn\n","qformer.encoder.layer.3.output\n","qformer.encoder.layer.3.output.dense\n","qformer.encoder.layer.3.output.LayerNorm\n","qformer.encoder.layer.3.output.dropout\n","qformer.encoder.layer.3.intermediate_query\n","qformer.encoder.layer.3.intermediate_query.dense\n","qformer.encoder.layer.3.intermediate_query.intermediate_act_fn\n","qformer.encoder.layer.3.output_query\n","qformer.encoder.layer.3.output_query.dense\n","qformer.encoder.layer.3.output_query.LayerNorm\n","qformer.encoder.layer.3.output_query.dropout\n","qformer.encoder.layer.4\n","qformer.encoder.layer.4.attention\n","qformer.encoder.layer.4.attention.attention\n","qformer.encoder.layer.4.attention.attention.query\n","qformer.encoder.layer.4.attention.attention.key\n","qformer.encoder.layer.4.attention.attention.value\n","qformer.encoder.layer.4.attention.attention.dropout\n","qformer.encoder.layer.4.attention.output\n","qformer.encoder.layer.4.attention.output.dense\n","qformer.encoder.layer.4.attention.output.LayerNorm\n","qformer.encoder.layer.4.attention.output.dropout\n","qformer.encoder.layer.4.crossattention\n","qformer.encoder.layer.4.crossattention.attention\n","qformer.encoder.layer.4.crossattention.attention.query\n","qformer.encoder.layer.4.crossattention.attention.key\n","qformer.encoder.layer.4.crossattention.attention.value\n","qformer.encoder.layer.4.crossattention.attention.dropout\n","qformer.encoder.layer.4.crossattention.output\n","qformer.encoder.layer.4.crossattention.output.dense\n","qformer.encoder.layer.4.crossattention.output.LayerNorm\n","qformer.encoder.layer.4.crossattention.output.dropout\n","qformer.encoder.layer.4.intermediate\n","qformer.encoder.layer.4.intermediate.dense\n","qformer.encoder.layer.4.intermediate.intermediate_act_fn\n","qformer.encoder.layer.4.output\n","qformer.encoder.layer.4.output.dense\n","qformer.encoder.layer.4.output.LayerNorm\n","qformer.encoder.layer.4.output.dropout\n","qformer.encoder.layer.4.intermediate_query\n","qformer.encoder.layer.4.intermediate_query.dense\n","qformer.encoder.layer.4.intermediate_query.intermediate_act_fn\n","qformer.encoder.layer.4.output_query\n","qformer.encoder.layer.4.output_query.dense\n","qformer.encoder.layer.4.output_query.LayerNorm\n","qformer.encoder.layer.4.output_query.dropout\n","qformer.encoder.layer.5\n","qformer.encoder.layer.5.attention\n","qformer.encoder.layer.5.attention.attention\n","qformer.encoder.layer.5.attention.attention.query\n","qformer.encoder.layer.5.attention.attention.key\n","qformer.encoder.layer.5.attention.attention.value\n","qformer.encoder.layer.5.attention.attention.dropout\n","qformer.encoder.layer.5.attention.output\n","qformer.encoder.layer.5.attention.output.dense\n","qformer.encoder.layer.5.attention.output.LayerNorm\n","qformer.encoder.layer.5.attention.output.dropout\n","qformer.encoder.layer.5.intermediate\n","qformer.encoder.layer.5.intermediate.dense\n","qformer.encoder.layer.5.intermediate.intermediate_act_fn\n","qformer.encoder.layer.5.output\n","qformer.encoder.layer.5.output.dense\n","qformer.encoder.layer.5.output.LayerNorm\n","qformer.encoder.layer.5.output.dropout\n","qformer.encoder.layer.5.intermediate_query\n","qformer.encoder.layer.5.intermediate_query.dense\n","qformer.encoder.layer.5.intermediate_query.intermediate_act_fn\n","qformer.encoder.layer.5.output_query\n","qformer.encoder.layer.5.output_query.dense\n","qformer.encoder.layer.5.output_query.LayerNorm\n","qformer.encoder.layer.5.output_query.dropout\n","qformer.encoder.layer.6\n","qformer.encoder.layer.6.attention\n","qformer.encoder.layer.6.attention.attention\n","qformer.encoder.layer.6.attention.attention.query\n","qformer.encoder.layer.6.attention.attention.key\n","qformer.encoder.layer.6.attention.attention.value\n","qformer.encoder.layer.6.attention.attention.dropout\n","qformer.encoder.layer.6.attention.output\n","qformer.encoder.layer.6.attention.output.dense\n","qformer.encoder.layer.6.attention.output.LayerNorm\n","qformer.encoder.layer.6.attention.output.dropout\n","qformer.encoder.layer.6.crossattention\n","qformer.encoder.layer.6.crossattention.attention\n","qformer.encoder.layer.6.crossattention.attention.query\n","qformer.encoder.layer.6.crossattention.attention.key\n","qformer.encoder.layer.6.crossattention.attention.value\n","qformer.encoder.layer.6.crossattention.attention.dropout\n","qformer.encoder.layer.6.crossattention.output\n","qformer.encoder.layer.6.crossattention.output.dense\n","qformer.encoder.layer.6.crossattention.output.LayerNorm\n","qformer.encoder.layer.6.crossattention.output.dropout\n","qformer.encoder.layer.6.intermediate\n","qformer.encoder.layer.6.intermediate.dense\n","qformer.encoder.layer.6.intermediate.intermediate_act_fn\n","qformer.encoder.layer.6.output\n","qformer.encoder.layer.6.output.dense\n","qformer.encoder.layer.6.output.LayerNorm\n","qformer.encoder.layer.6.output.dropout\n","qformer.encoder.layer.6.intermediate_query\n","qformer.encoder.layer.6.intermediate_query.dense\n","qformer.encoder.layer.6.intermediate_query.intermediate_act_fn\n","qformer.encoder.layer.6.output_query\n","qformer.encoder.layer.6.output_query.dense\n","qformer.encoder.layer.6.output_query.LayerNorm\n","qformer.encoder.layer.6.output_query.dropout\n","qformer.encoder.layer.7\n","qformer.encoder.layer.7.attention\n","qformer.encoder.layer.7.attention.attention\n","qformer.encoder.layer.7.attention.attention.query\n","qformer.encoder.layer.7.attention.attention.key\n","qformer.encoder.layer.7.attention.attention.value\n","qformer.encoder.layer.7.attention.attention.dropout\n","qformer.encoder.layer.7.attention.output\n","qformer.encoder.layer.7.attention.output.dense\n","qformer.encoder.layer.7.attention.output.LayerNorm\n","qformer.encoder.layer.7.attention.output.dropout\n","qformer.encoder.layer.7.intermediate\n","qformer.encoder.layer.7.intermediate.dense\n","qformer.encoder.layer.7.intermediate.intermediate_act_fn\n","qformer.encoder.layer.7.output\n","qformer.encoder.layer.7.output.dense\n","qformer.encoder.layer.7.output.LayerNorm\n","qformer.encoder.layer.7.output.dropout\n","qformer.encoder.layer.7.intermediate_query\n","qformer.encoder.layer.7.intermediate_query.dense\n","qformer.encoder.layer.7.intermediate_query.intermediate_act_fn\n","qformer.encoder.layer.7.output_query\n","qformer.encoder.layer.7.output_query.dense\n","qformer.encoder.layer.7.output_query.LayerNorm\n","qformer.encoder.layer.7.output_query.dropout\n","qformer.encoder.layer.8\n","qformer.encoder.layer.8.attention\n","qformer.encoder.layer.8.attention.attention\n","qformer.encoder.layer.8.attention.attention.query\n","qformer.encoder.layer.8.attention.attention.key\n","qformer.encoder.layer.8.attention.attention.value\n","qformer.encoder.layer.8.attention.attention.dropout\n","qformer.encoder.layer.8.attention.output\n","qformer.encoder.layer.8.attention.output.dense\n","qformer.encoder.layer.8.attention.output.LayerNorm\n","qformer.encoder.layer.8.attention.output.dropout\n","qformer.encoder.layer.8.crossattention\n","qformer.encoder.layer.8.crossattention.attention\n","qformer.encoder.layer.8.crossattention.attention.query\n","qformer.encoder.layer.8.crossattention.attention.key\n","qformer.encoder.layer.8.crossattention.attention.value\n","qformer.encoder.layer.8.crossattention.attention.dropout\n","qformer.encoder.layer.8.crossattention.output\n","qformer.encoder.layer.8.crossattention.output.dense\n","qformer.encoder.layer.8.crossattention.output.LayerNorm\n","qformer.encoder.layer.8.crossattention.output.dropout\n","qformer.encoder.layer.8.intermediate\n","qformer.encoder.layer.8.intermediate.dense\n","qformer.encoder.layer.8.intermediate.intermediate_act_fn\n","qformer.encoder.layer.8.output\n","qformer.encoder.layer.8.output.dense\n","qformer.encoder.layer.8.output.LayerNorm\n","qformer.encoder.layer.8.output.dropout\n","qformer.encoder.layer.8.intermediate_query\n","qformer.encoder.layer.8.intermediate_query.dense\n","qformer.encoder.layer.8.intermediate_query.intermediate_act_fn\n","qformer.encoder.layer.8.output_query\n","qformer.encoder.layer.8.output_query.dense\n","qformer.encoder.layer.8.output_query.LayerNorm\n","qformer.encoder.layer.8.output_query.dropout\n","qformer.encoder.layer.9\n","qformer.encoder.layer.9.attention\n","qformer.encoder.layer.9.attention.attention\n","qformer.encoder.layer.9.attention.attention.query\n","qformer.encoder.layer.9.attention.attention.key\n","qformer.encoder.layer.9.attention.attention.value\n","qformer.encoder.layer.9.attention.attention.dropout\n","qformer.encoder.layer.9.attention.output\n","qformer.encoder.layer.9.attention.output.dense\n","qformer.encoder.layer.9.attention.output.LayerNorm\n","qformer.encoder.layer.9.attention.output.dropout\n","qformer.encoder.layer.9.intermediate\n","qformer.encoder.layer.9.intermediate.dense\n","qformer.encoder.layer.9.intermediate.intermediate_act_fn\n","qformer.encoder.layer.9.output\n","qformer.encoder.layer.9.output.dense\n","qformer.encoder.layer.9.output.LayerNorm\n","qformer.encoder.layer.9.output.dropout\n","qformer.encoder.layer.9.intermediate_query\n","qformer.encoder.layer.9.intermediate_query.dense\n","qformer.encoder.layer.9.intermediate_query.intermediate_act_fn\n","qformer.encoder.layer.9.output_query\n","qformer.encoder.layer.9.output_query.dense\n","qformer.encoder.layer.9.output_query.LayerNorm\n","qformer.encoder.layer.9.output_query.dropout\n","qformer.encoder.layer.10\n","qformer.encoder.layer.10.attention\n","qformer.encoder.layer.10.attention.attention\n","qformer.encoder.layer.10.attention.attention.query\n","qformer.encoder.layer.10.attention.attention.key\n","qformer.encoder.layer.10.attention.attention.value\n","qformer.encoder.layer.10.attention.attention.dropout\n","qformer.encoder.layer.10.attention.output\n","qformer.encoder.layer.10.attention.output.dense\n","qformer.encoder.layer.10.attention.output.LayerNorm\n","qformer.encoder.layer.10.attention.output.dropout\n","qformer.encoder.layer.10.crossattention\n","qformer.encoder.layer.10.crossattention.attention\n","qformer.encoder.layer.10.crossattention.attention.query\n","qformer.encoder.layer.10.crossattention.attention.key\n","qformer.encoder.layer.10.crossattention.attention.value\n","qformer.encoder.layer.10.crossattention.attention.dropout\n","qformer.encoder.layer.10.crossattention.output\n","qformer.encoder.layer.10.crossattention.output.dense\n","qformer.encoder.layer.10.crossattention.output.LayerNorm\n","qformer.encoder.layer.10.crossattention.output.dropout\n","qformer.encoder.layer.10.intermediate\n","qformer.encoder.layer.10.intermediate.dense\n","qformer.encoder.layer.10.intermediate.intermediate_act_fn\n","qformer.encoder.layer.10.output\n","qformer.encoder.layer.10.output.dense\n","qformer.encoder.layer.10.output.LayerNorm\n","qformer.encoder.layer.10.output.dropout\n","qformer.encoder.layer.10.intermediate_query\n","qformer.encoder.layer.10.intermediate_query.dense\n","qformer.encoder.layer.10.intermediate_query.intermediate_act_fn\n","qformer.encoder.layer.10.output_query\n","qformer.encoder.layer.10.output_query.dense\n","qformer.encoder.layer.10.output_query.LayerNorm\n","qformer.encoder.layer.10.output_query.dropout\n","qformer.encoder.layer.11\n","qformer.encoder.layer.11.attention\n","qformer.encoder.layer.11.attention.attention\n","qformer.encoder.layer.11.attention.attention.query\n","qformer.encoder.layer.11.attention.attention.key\n","qformer.encoder.layer.11.attention.attention.value\n","qformer.encoder.layer.11.attention.attention.dropout\n","qformer.encoder.layer.11.attention.output\n","qformer.encoder.layer.11.attention.output.dense\n","qformer.encoder.layer.11.attention.output.LayerNorm\n","qformer.encoder.layer.11.attention.output.dropout\n","qformer.encoder.layer.11.intermediate\n","qformer.encoder.layer.11.intermediate.dense\n","qformer.encoder.layer.11.intermediate.intermediate_act_fn\n","qformer.encoder.layer.11.output\n","qformer.encoder.layer.11.output.dense\n","qformer.encoder.layer.11.output.LayerNorm\n","qformer.encoder.layer.11.output.dropout\n","qformer.encoder.layer.11.intermediate_query\n","qformer.encoder.layer.11.intermediate_query.dense\n","qformer.encoder.layer.11.intermediate_query.intermediate_act_fn\n","qformer.encoder.layer.11.output_query\n","qformer.encoder.layer.11.output_query.dense\n","qformer.encoder.layer.11.output_query.LayerNorm\n","qformer.encoder.layer.11.output_query.dropout\n","language_projection\n","language_model\n","language_model.model\n","language_model.model.embed_tokens\n","language_model.model.layers\n","language_model.model.layers.0\n","language_model.model.layers.0.self_attn\n","language_model.model.layers.0.self_attn.q_proj\n","language_model.model.layers.0.self_attn.k_proj\n","language_model.model.layers.0.self_attn.v_proj\n","language_model.model.layers.0.self_attn.o_proj\n","language_model.model.layers.0.self_attn.rotary_emb\n","language_model.model.layers.0.mlp\n","language_model.model.layers.0.mlp.gate_proj\n","language_model.model.layers.0.mlp.up_proj\n","language_model.model.layers.0.mlp.down_proj\n","language_model.model.layers.0.mlp.act_fn\n","language_model.model.layers.0.input_layernorm\n","language_model.model.layers.0.post_attention_layernorm\n","language_model.model.layers.1\n","language_model.model.layers.1.self_attn\n","language_model.model.layers.1.self_attn.q_proj\n","language_model.model.layers.1.self_attn.k_proj\n","language_model.model.layers.1.self_attn.v_proj\n","language_model.model.layers.1.self_attn.o_proj\n","language_model.model.layers.1.self_attn.rotary_emb\n","language_model.model.layers.1.mlp\n","language_model.model.layers.1.mlp.gate_proj\n","language_model.model.layers.1.mlp.up_proj\n","language_model.model.layers.1.mlp.down_proj\n","language_model.model.layers.1.mlp.act_fn\n","language_model.model.layers.1.input_layernorm\n","language_model.model.layers.1.post_attention_layernorm\n","language_model.model.layers.2\n","language_model.model.layers.2.self_attn\n","language_model.model.layers.2.self_attn.q_proj\n","language_model.model.layers.2.self_attn.k_proj\n","language_model.model.layers.2.self_attn.v_proj\n","language_model.model.layers.2.self_attn.o_proj\n","language_model.model.layers.2.self_attn.rotary_emb\n","language_model.model.layers.2.mlp\n","language_model.model.layers.2.mlp.gate_proj\n","language_model.model.layers.2.mlp.up_proj\n","language_model.model.layers.2.mlp.down_proj\n","language_model.model.layers.2.mlp.act_fn\n","language_model.model.layers.2.input_layernorm\n","language_model.model.layers.2.post_attention_layernorm\n","language_model.model.layers.3\n","language_model.model.layers.3.self_attn\n","language_model.model.layers.3.self_attn.q_proj\n","language_model.model.layers.3.self_attn.k_proj\n","language_model.model.layers.3.self_attn.v_proj\n","language_model.model.layers.3.self_attn.o_proj\n","language_model.model.layers.3.self_attn.rotary_emb\n","language_model.model.layers.3.mlp\n","language_model.model.layers.3.mlp.gate_proj\n","language_model.model.layers.3.mlp.up_proj\n","language_model.model.layers.3.mlp.down_proj\n","language_model.model.layers.3.mlp.act_fn\n","language_model.model.layers.3.input_layernorm\n","language_model.model.layers.3.post_attention_layernorm\n","language_model.model.layers.4\n","language_model.model.layers.4.self_attn\n","language_model.model.layers.4.self_attn.q_proj\n","language_model.model.layers.4.self_attn.k_proj\n","language_model.model.layers.4.self_attn.v_proj\n","language_model.model.layers.4.self_attn.o_proj\n","language_model.model.layers.4.self_attn.rotary_emb\n","language_model.model.layers.4.mlp\n","language_model.model.layers.4.mlp.gate_proj\n","language_model.model.layers.4.mlp.up_proj\n","language_model.model.layers.4.mlp.down_proj\n","language_model.model.layers.4.mlp.act_fn\n","language_model.model.layers.4.input_layernorm\n","language_model.model.layers.4.post_attention_layernorm\n","language_model.model.layers.5\n","language_model.model.layers.5.self_attn\n","language_model.model.layers.5.self_attn.q_proj\n","language_model.model.layers.5.self_attn.k_proj\n","language_model.model.layers.5.self_attn.v_proj\n","language_model.model.layers.5.self_attn.o_proj\n","language_model.model.layers.5.self_attn.rotary_emb\n","language_model.model.layers.5.mlp\n","language_model.model.layers.5.mlp.gate_proj\n","language_model.model.layers.5.mlp.up_proj\n","language_model.model.layers.5.mlp.down_proj\n","language_model.model.layers.5.mlp.act_fn\n","language_model.model.layers.5.input_layernorm\n","language_model.model.layers.5.post_attention_layernorm\n","language_model.model.layers.6\n","language_model.model.layers.6.self_attn\n","language_model.model.layers.6.self_attn.q_proj\n","language_model.model.layers.6.self_attn.k_proj\n","language_model.model.layers.6.self_attn.v_proj\n","language_model.model.layers.6.self_attn.o_proj\n","language_model.model.layers.6.self_attn.rotary_emb\n","language_model.model.layers.6.mlp\n","language_model.model.layers.6.mlp.gate_proj\n","language_model.model.layers.6.mlp.up_proj\n","language_model.model.layers.6.mlp.down_proj\n","language_model.model.layers.6.mlp.act_fn\n","language_model.model.layers.6.input_layernorm\n","language_model.model.layers.6.post_attention_layernorm\n","language_model.model.layers.7\n","language_model.model.layers.7.self_attn\n","language_model.model.layers.7.self_attn.q_proj\n","language_model.model.layers.7.self_attn.k_proj\n","language_model.model.layers.7.self_attn.v_proj\n","language_model.model.layers.7.self_attn.o_proj\n","language_model.model.layers.7.self_attn.rotary_emb\n","language_model.model.layers.7.mlp\n","language_model.model.layers.7.mlp.gate_proj\n","language_model.model.layers.7.mlp.up_proj\n","language_model.model.layers.7.mlp.down_proj\n","language_model.model.layers.7.mlp.act_fn\n","language_model.model.layers.7.input_layernorm\n","language_model.model.layers.7.post_attention_layernorm\n","language_model.model.layers.8\n","language_model.model.layers.8.self_attn\n","language_model.model.layers.8.self_attn.q_proj\n","language_model.model.layers.8.self_attn.k_proj\n","language_model.model.layers.8.self_attn.v_proj\n","language_model.model.layers.8.self_attn.o_proj\n","language_model.model.layers.8.self_attn.rotary_emb\n","language_model.model.layers.8.mlp\n","language_model.model.layers.8.mlp.gate_proj\n","language_model.model.layers.8.mlp.up_proj\n","language_model.model.layers.8.mlp.down_proj\n","language_model.model.layers.8.mlp.act_fn\n","language_model.model.layers.8.input_layernorm\n","language_model.model.layers.8.post_attention_layernorm\n","language_model.model.layers.9\n","language_model.model.layers.9.self_attn\n","language_model.model.layers.9.self_attn.q_proj\n","language_model.model.layers.9.self_attn.k_proj\n","language_model.model.layers.9.self_attn.v_proj\n","language_model.model.layers.9.self_attn.o_proj\n","language_model.model.layers.9.self_attn.rotary_emb\n","language_model.model.layers.9.mlp\n","language_model.model.layers.9.mlp.gate_proj\n","language_model.model.layers.9.mlp.up_proj\n","language_model.model.layers.9.mlp.down_proj\n","language_model.model.layers.9.mlp.act_fn\n","language_model.model.layers.9.input_layernorm\n","language_model.model.layers.9.post_attention_layernorm\n","language_model.model.layers.10\n","language_model.model.layers.10.self_attn\n","language_model.model.layers.10.self_attn.q_proj\n","language_model.model.layers.10.self_attn.k_proj\n","language_model.model.layers.10.self_attn.v_proj\n","language_model.model.layers.10.self_attn.o_proj\n","language_model.model.layers.10.self_attn.rotary_emb\n","language_model.model.layers.10.mlp\n","language_model.model.layers.10.mlp.gate_proj\n","language_model.model.layers.10.mlp.up_proj\n","language_model.model.layers.10.mlp.down_proj\n","language_model.model.layers.10.mlp.act_fn\n","language_model.model.layers.10.input_layernorm\n","language_model.model.layers.10.post_attention_layernorm\n","language_model.model.layers.11\n","language_model.model.layers.11.self_attn\n","language_model.model.layers.11.self_attn.q_proj\n","language_model.model.layers.11.self_attn.k_proj\n","language_model.model.layers.11.self_attn.v_proj\n","language_model.model.layers.11.self_attn.o_proj\n","language_model.model.layers.11.self_attn.rotary_emb\n","language_model.model.layers.11.mlp\n","language_model.model.layers.11.mlp.gate_proj\n","language_model.model.layers.11.mlp.up_proj\n","language_model.model.layers.11.mlp.down_proj\n","language_model.model.layers.11.mlp.act_fn\n","language_model.model.layers.11.input_layernorm\n","language_model.model.layers.11.post_attention_layernorm\n","language_model.model.layers.12\n","language_model.model.layers.12.self_attn\n","language_model.model.layers.12.self_attn.q_proj\n","language_model.model.layers.12.self_attn.k_proj\n","language_model.model.layers.12.self_attn.v_proj\n","language_model.model.layers.12.self_attn.o_proj\n","language_model.model.layers.12.self_attn.rotary_emb\n","language_model.model.layers.12.mlp\n","language_model.model.layers.12.mlp.gate_proj\n","language_model.model.layers.12.mlp.up_proj\n","language_model.model.layers.12.mlp.down_proj\n","language_model.model.layers.12.mlp.act_fn\n","language_model.model.layers.12.input_layernorm\n","language_model.model.layers.12.post_attention_layernorm\n","language_model.model.layers.13\n","language_model.model.layers.13.self_attn\n","language_model.model.layers.13.self_attn.q_proj\n","language_model.model.layers.13.self_attn.k_proj\n","language_model.model.layers.13.self_attn.v_proj\n","language_model.model.layers.13.self_attn.o_proj\n","language_model.model.layers.13.self_attn.rotary_emb\n","language_model.model.layers.13.mlp\n","language_model.model.layers.13.mlp.gate_proj\n","language_model.model.layers.13.mlp.up_proj\n","language_model.model.layers.13.mlp.down_proj\n","language_model.model.layers.13.mlp.act_fn\n","language_model.model.layers.13.input_layernorm\n","language_model.model.layers.13.post_attention_layernorm\n","language_model.model.layers.14\n","language_model.model.layers.14.self_attn\n","language_model.model.layers.14.self_attn.q_proj\n","language_model.model.layers.14.self_attn.k_proj\n","language_model.model.layers.14.self_attn.v_proj\n","language_model.model.layers.14.self_attn.o_proj\n","language_model.model.layers.14.self_attn.rotary_emb\n","language_model.model.layers.14.mlp\n","language_model.model.layers.14.mlp.gate_proj\n","language_model.model.layers.14.mlp.up_proj\n","language_model.model.layers.14.mlp.down_proj\n","language_model.model.layers.14.mlp.act_fn\n","language_model.model.layers.14.input_layernorm\n","language_model.model.layers.14.post_attention_layernorm\n","language_model.model.layers.15\n","language_model.model.layers.15.self_attn\n","language_model.model.layers.15.self_attn.q_proj\n","language_model.model.layers.15.self_attn.k_proj\n","language_model.model.layers.15.self_attn.v_proj\n","language_model.model.layers.15.self_attn.o_proj\n","language_model.model.layers.15.self_attn.rotary_emb\n","language_model.model.layers.15.mlp\n","language_model.model.layers.15.mlp.gate_proj\n","language_model.model.layers.15.mlp.up_proj\n","language_model.model.layers.15.mlp.down_proj\n","language_model.model.layers.15.mlp.act_fn\n","language_model.model.layers.15.input_layernorm\n","language_model.model.layers.15.post_attention_layernorm\n","language_model.model.layers.16\n","language_model.model.layers.16.self_attn\n","language_model.model.layers.16.self_attn.q_proj\n","language_model.model.layers.16.self_attn.k_proj\n","language_model.model.layers.16.self_attn.v_proj\n","language_model.model.layers.16.self_attn.o_proj\n","language_model.model.layers.16.self_attn.rotary_emb\n","language_model.model.layers.16.mlp\n","language_model.model.layers.16.mlp.gate_proj\n","language_model.model.layers.16.mlp.up_proj\n","language_model.model.layers.16.mlp.down_proj\n","language_model.model.layers.16.mlp.act_fn\n","language_model.model.layers.16.input_layernorm\n","language_model.model.layers.16.post_attention_layernorm\n","language_model.model.layers.17\n","language_model.model.layers.17.self_attn\n","language_model.model.layers.17.self_attn.q_proj\n","language_model.model.layers.17.self_attn.k_proj\n","language_model.model.layers.17.self_attn.v_proj\n","language_model.model.layers.17.self_attn.o_proj\n","language_model.model.layers.17.self_attn.rotary_emb\n","language_model.model.layers.17.mlp\n","language_model.model.layers.17.mlp.gate_proj\n","language_model.model.layers.17.mlp.up_proj\n","language_model.model.layers.17.mlp.down_proj\n","language_model.model.layers.17.mlp.act_fn\n","language_model.model.layers.17.input_layernorm\n","language_model.model.layers.17.post_attention_layernorm\n","language_model.model.layers.18\n","language_model.model.layers.18.self_attn\n","language_model.model.layers.18.self_attn.q_proj\n","language_model.model.layers.18.self_attn.k_proj\n","language_model.model.layers.18.self_attn.v_proj\n","language_model.model.layers.18.self_attn.o_proj\n","language_model.model.layers.18.self_attn.rotary_emb\n","language_model.model.layers.18.mlp\n","language_model.model.layers.18.mlp.gate_proj\n","language_model.model.layers.18.mlp.up_proj\n","language_model.model.layers.18.mlp.down_proj\n","language_model.model.layers.18.mlp.act_fn\n","language_model.model.layers.18.input_layernorm\n","language_model.model.layers.18.post_attention_layernorm\n","language_model.model.layers.19\n","language_model.model.layers.19.self_attn\n","language_model.model.layers.19.self_attn.q_proj\n","language_model.model.layers.19.self_attn.k_proj\n","language_model.model.layers.19.self_attn.v_proj\n","language_model.model.layers.19.self_attn.o_proj\n","language_model.model.layers.19.self_attn.rotary_emb\n","language_model.model.layers.19.mlp\n","language_model.model.layers.19.mlp.gate_proj\n","language_model.model.layers.19.mlp.up_proj\n","language_model.model.layers.19.mlp.down_proj\n","language_model.model.layers.19.mlp.act_fn\n","language_model.model.layers.19.input_layernorm\n","language_model.model.layers.19.post_attention_layernorm\n","language_model.model.layers.20\n","language_model.model.layers.20.self_attn\n","language_model.model.layers.20.self_attn.q_proj\n","language_model.model.layers.20.self_attn.k_proj\n","language_model.model.layers.20.self_attn.v_proj\n","language_model.model.layers.20.self_attn.o_proj\n","language_model.model.layers.20.self_attn.rotary_emb\n","language_model.model.layers.20.mlp\n","language_model.model.layers.20.mlp.gate_proj\n","language_model.model.layers.20.mlp.up_proj\n","language_model.model.layers.20.mlp.down_proj\n","language_model.model.layers.20.mlp.act_fn\n","language_model.model.layers.20.input_layernorm\n","language_model.model.layers.20.post_attention_layernorm\n","language_model.model.layers.21\n","language_model.model.layers.21.self_attn\n","language_model.model.layers.21.self_attn.q_proj\n","language_model.model.layers.21.self_attn.k_proj\n","language_model.model.layers.21.self_attn.v_proj\n","language_model.model.layers.21.self_attn.o_proj\n","language_model.model.layers.21.self_attn.rotary_emb\n","language_model.model.layers.21.mlp\n","language_model.model.layers.21.mlp.gate_proj\n","language_model.model.layers.21.mlp.up_proj\n","language_model.model.layers.21.mlp.down_proj\n","language_model.model.layers.21.mlp.act_fn\n","language_model.model.layers.21.input_layernorm\n","language_model.model.layers.21.post_attention_layernorm\n","language_model.model.layers.22\n","language_model.model.layers.22.self_attn\n","language_model.model.layers.22.self_attn.q_proj\n","language_model.model.layers.22.self_attn.k_proj\n","language_model.model.layers.22.self_attn.v_proj\n","language_model.model.layers.22.self_attn.o_proj\n","language_model.model.layers.22.self_attn.rotary_emb\n","language_model.model.layers.22.mlp\n","language_model.model.layers.22.mlp.gate_proj\n","language_model.model.layers.22.mlp.up_proj\n","language_model.model.layers.22.mlp.down_proj\n","language_model.model.layers.22.mlp.act_fn\n","language_model.model.layers.22.input_layernorm\n","language_model.model.layers.22.post_attention_layernorm\n","language_model.model.layers.23\n","language_model.model.layers.23.self_attn\n","language_model.model.layers.23.self_attn.q_proj\n","language_model.model.layers.23.self_attn.k_proj\n","language_model.model.layers.23.self_attn.v_proj\n","language_model.model.layers.23.self_attn.o_proj\n","language_model.model.layers.23.self_attn.rotary_emb\n","language_model.model.layers.23.mlp\n","language_model.model.layers.23.mlp.gate_proj\n","language_model.model.layers.23.mlp.up_proj\n","language_model.model.layers.23.mlp.down_proj\n","language_model.model.layers.23.mlp.act_fn\n","language_model.model.layers.23.input_layernorm\n","language_model.model.layers.23.post_attention_layernorm\n","language_model.model.layers.24\n","language_model.model.layers.24.self_attn\n","language_model.model.layers.24.self_attn.q_proj\n","language_model.model.layers.24.self_attn.k_proj\n","language_model.model.layers.24.self_attn.v_proj\n","language_model.model.layers.24.self_attn.o_proj\n","language_model.model.layers.24.self_attn.rotary_emb\n","language_model.model.layers.24.mlp\n","language_model.model.layers.24.mlp.gate_proj\n","language_model.model.layers.24.mlp.up_proj\n","language_model.model.layers.24.mlp.down_proj\n","language_model.model.layers.24.mlp.act_fn\n","language_model.model.layers.24.input_layernorm\n","language_model.model.layers.24.post_attention_layernorm\n","language_model.model.layers.25\n","language_model.model.layers.25.self_attn\n","language_model.model.layers.25.self_attn.q_proj\n","language_model.model.layers.25.self_attn.k_proj\n","language_model.model.layers.25.self_attn.v_proj\n","language_model.model.layers.25.self_attn.o_proj\n","language_model.model.layers.25.self_attn.rotary_emb\n","language_model.model.layers.25.mlp\n","language_model.model.layers.25.mlp.gate_proj\n","language_model.model.layers.25.mlp.up_proj\n","language_model.model.layers.25.mlp.down_proj\n","language_model.model.layers.25.mlp.act_fn\n","language_model.model.layers.25.input_layernorm\n","language_model.model.layers.25.post_attention_layernorm\n","language_model.model.layers.26\n","language_model.model.layers.26.self_attn\n","language_model.model.layers.26.self_attn.q_proj\n","language_model.model.layers.26.self_attn.k_proj\n","language_model.model.layers.26.self_attn.v_proj\n","language_model.model.layers.26.self_attn.o_proj\n","language_model.model.layers.26.self_attn.rotary_emb\n","language_model.model.layers.26.mlp\n","language_model.model.layers.26.mlp.gate_proj\n","language_model.model.layers.26.mlp.up_proj\n","language_model.model.layers.26.mlp.down_proj\n","language_model.model.layers.26.mlp.act_fn\n","language_model.model.layers.26.input_layernorm\n","language_model.model.layers.26.post_attention_layernorm\n","language_model.model.layers.27\n","language_model.model.layers.27.self_attn\n","language_model.model.layers.27.self_attn.q_proj\n","language_model.model.layers.27.self_attn.k_proj\n","language_model.model.layers.27.self_attn.v_proj\n","language_model.model.layers.27.self_attn.o_proj\n","language_model.model.layers.27.self_attn.rotary_emb\n","language_model.model.layers.27.mlp\n","language_model.model.layers.27.mlp.gate_proj\n","language_model.model.layers.27.mlp.up_proj\n","language_model.model.layers.27.mlp.down_proj\n","language_model.model.layers.27.mlp.act_fn\n","language_model.model.layers.27.input_layernorm\n","language_model.model.layers.27.post_attention_layernorm\n","language_model.model.layers.28\n","language_model.model.layers.28.self_attn\n","language_model.model.layers.28.self_attn.q_proj\n","language_model.model.layers.28.self_attn.k_proj\n","language_model.model.layers.28.self_attn.v_proj\n","language_model.model.layers.28.self_attn.o_proj\n","language_model.model.layers.28.self_attn.rotary_emb\n","language_model.model.layers.28.mlp\n","language_model.model.layers.28.mlp.gate_proj\n","language_model.model.layers.28.mlp.up_proj\n","language_model.model.layers.28.mlp.down_proj\n","language_model.model.layers.28.mlp.act_fn\n","language_model.model.layers.28.input_layernorm\n","language_model.model.layers.28.post_attention_layernorm\n","language_model.model.layers.29\n","language_model.model.layers.29.self_attn\n","language_model.model.layers.29.self_attn.q_proj\n","language_model.model.layers.29.self_attn.k_proj\n","language_model.model.layers.29.self_attn.v_proj\n","language_model.model.layers.29.self_attn.o_proj\n","language_model.model.layers.29.self_attn.rotary_emb\n","language_model.model.layers.29.mlp\n","language_model.model.layers.29.mlp.gate_proj\n","language_model.model.layers.29.mlp.up_proj\n","language_model.model.layers.29.mlp.down_proj\n","language_model.model.layers.29.mlp.act_fn\n","language_model.model.layers.29.input_layernorm\n","language_model.model.layers.29.post_attention_layernorm\n","language_model.model.layers.30\n","language_model.model.layers.30.self_attn\n","language_model.model.layers.30.self_attn.q_proj\n","language_model.model.layers.30.self_attn.k_proj\n","language_model.model.layers.30.self_attn.v_proj\n","language_model.model.layers.30.self_attn.o_proj\n","language_model.model.layers.30.self_attn.rotary_emb\n","language_model.model.layers.30.mlp\n","language_model.model.layers.30.mlp.gate_proj\n","language_model.model.layers.30.mlp.up_proj\n","language_model.model.layers.30.mlp.down_proj\n","language_model.model.layers.30.mlp.act_fn\n","language_model.model.layers.30.input_layernorm\n","language_model.model.layers.30.post_attention_layernorm\n","language_model.model.layers.31\n","language_model.model.layers.31.self_attn\n","language_model.model.layers.31.self_attn.q_proj\n","language_model.model.layers.31.self_attn.k_proj\n","language_model.model.layers.31.self_attn.v_proj\n","language_model.model.layers.31.self_attn.o_proj\n","language_model.model.layers.31.self_attn.rotary_emb\n","language_model.model.layers.31.mlp\n","language_model.model.layers.31.mlp.gate_proj\n","language_model.model.layers.31.mlp.up_proj\n","language_model.model.layers.31.mlp.down_proj\n","language_model.model.layers.31.mlp.act_fn\n","language_model.model.layers.31.input_layernorm\n","language_model.model.layers.31.post_attention_layernorm\n","language_model.model.layers.32\n","language_model.model.layers.32.self_attn\n","language_model.model.layers.32.self_attn.q_proj\n","language_model.model.layers.32.self_attn.k_proj\n","language_model.model.layers.32.self_attn.v_proj\n","language_model.model.layers.32.self_attn.o_proj\n","language_model.model.layers.32.self_attn.rotary_emb\n","language_model.model.layers.32.mlp\n","language_model.model.layers.32.mlp.gate_proj\n","language_model.model.layers.32.mlp.up_proj\n","language_model.model.layers.32.mlp.down_proj\n","language_model.model.layers.32.mlp.act_fn\n","language_model.model.layers.32.input_layernorm\n","language_model.model.layers.32.post_attention_layernorm\n","language_model.model.layers.33\n","language_model.model.layers.33.self_attn\n","language_model.model.layers.33.self_attn.q_proj\n","language_model.model.layers.33.self_attn.k_proj\n","language_model.model.layers.33.self_attn.v_proj\n","language_model.model.layers.33.self_attn.o_proj\n","language_model.model.layers.33.self_attn.rotary_emb\n","language_model.model.layers.33.mlp\n","language_model.model.layers.33.mlp.gate_proj\n","language_model.model.layers.33.mlp.up_proj\n","language_model.model.layers.33.mlp.down_proj\n","language_model.model.layers.33.mlp.act_fn\n","language_model.model.layers.33.input_layernorm\n","language_model.model.layers.33.post_attention_layernorm\n","language_model.model.layers.34\n","language_model.model.layers.34.self_attn\n","language_model.model.layers.34.self_attn.q_proj\n","language_model.model.layers.34.self_attn.k_proj\n","language_model.model.layers.34.self_attn.v_proj\n","language_model.model.layers.34.self_attn.o_proj\n","language_model.model.layers.34.self_attn.rotary_emb\n","language_model.model.layers.34.mlp\n","language_model.model.layers.34.mlp.gate_proj\n","language_model.model.layers.34.mlp.up_proj\n","language_model.model.layers.34.mlp.down_proj\n","language_model.model.layers.34.mlp.act_fn\n","language_model.model.layers.34.input_layernorm\n","language_model.model.layers.34.post_attention_layernorm\n","language_model.model.layers.35\n","language_model.model.layers.35.self_attn\n","language_model.model.layers.35.self_attn.q_proj\n","language_model.model.layers.35.self_attn.k_proj\n","language_model.model.layers.35.self_attn.v_proj\n","language_model.model.layers.35.self_attn.o_proj\n","language_model.model.layers.35.self_attn.rotary_emb\n","language_model.model.layers.35.mlp\n","language_model.model.layers.35.mlp.gate_proj\n","language_model.model.layers.35.mlp.up_proj\n","language_model.model.layers.35.mlp.down_proj\n","language_model.model.layers.35.mlp.act_fn\n","language_model.model.layers.35.input_layernorm\n","language_model.model.layers.35.post_attention_layernorm\n","language_model.model.layers.36\n","language_model.model.layers.36.self_attn\n","language_model.model.layers.36.self_attn.q_proj\n","language_model.model.layers.36.self_attn.k_proj\n","language_model.model.layers.36.self_attn.v_proj\n","language_model.model.layers.36.self_attn.o_proj\n","language_model.model.layers.36.self_attn.rotary_emb\n","language_model.model.layers.36.mlp\n","language_model.model.layers.36.mlp.gate_proj\n","language_model.model.layers.36.mlp.up_proj\n","language_model.model.layers.36.mlp.down_proj\n","language_model.model.layers.36.mlp.act_fn\n","language_model.model.layers.36.input_layernorm\n","language_model.model.layers.36.post_attention_layernorm\n","language_model.model.layers.37\n","language_model.model.layers.37.self_attn\n","language_model.model.layers.37.self_attn.q_proj\n","language_model.model.layers.37.self_attn.k_proj\n","language_model.model.layers.37.self_attn.v_proj\n","language_model.model.layers.37.self_attn.o_proj\n","language_model.model.layers.37.self_attn.rotary_emb\n","language_model.model.layers.37.mlp\n","language_model.model.layers.37.mlp.gate_proj\n","language_model.model.layers.37.mlp.up_proj\n","language_model.model.layers.37.mlp.down_proj\n","language_model.model.layers.37.mlp.act_fn\n","language_model.model.layers.37.input_layernorm\n","language_model.model.layers.37.post_attention_layernorm\n","language_model.model.layers.38\n","language_model.model.layers.38.self_attn\n","language_model.model.layers.38.self_attn.q_proj\n","language_model.model.layers.38.self_attn.k_proj\n","language_model.model.layers.38.self_attn.v_proj\n","language_model.model.layers.38.self_attn.o_proj\n","language_model.model.layers.38.self_attn.rotary_emb\n","language_model.model.layers.38.mlp\n","language_model.model.layers.38.mlp.gate_proj\n","language_model.model.layers.38.mlp.up_proj\n","language_model.model.layers.38.mlp.down_proj\n","language_model.model.layers.38.mlp.act_fn\n","language_model.model.layers.38.input_layernorm\n","language_model.model.layers.38.post_attention_layernorm\n","language_model.model.layers.39\n","language_model.model.layers.39.self_attn\n","language_model.model.layers.39.self_attn.q_proj\n","language_model.model.layers.39.self_attn.k_proj\n","language_model.model.layers.39.self_attn.v_proj\n","language_model.model.layers.39.self_attn.o_proj\n","language_model.model.layers.39.self_attn.rotary_emb\n","language_model.model.layers.39.mlp\n","language_model.model.layers.39.mlp.gate_proj\n","language_model.model.layers.39.mlp.up_proj\n","language_model.model.layers.39.mlp.down_proj\n","language_model.model.layers.39.mlp.act_fn\n","language_model.model.layers.39.input_layernorm\n","language_model.model.layers.39.post_attention_layernorm\n","language_model.model.norm\n","language_model.model.rotary_emb\n","language_model.lm_head\n"]}],"source":["for name, module in model.named_modules():\n","    print(name)"]},{"cell_type":"markdown","metadata":{"id":"VJR8ZYjv-64b"},"source":["## Checking Trainable Parameters\n","> This code section must not be modified! Any modifications may affect your evaluation. This section directly relates to the number of trainable parameters, which is one of the grading criteria. If any false reporting regarding this section is detected, it may result in penalties during evaluation."]},{"cell_type":"code","source":["def disable_trainable_parameters(model):\n","    for param in model.parameters():\n","        param.requires_grad = False  # 모든 파라미터를 비학습 상태로 설정\n","    model.eval()  # 모델을 평가 모드로 전환\n","\n","disable_trainable_parameters(model)"],"metadata":{"id":"rzre9ANekOQ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88yX8b81eHqP","outputId":"8eee713e-461f-434b-a466-a439756664db"},"outputs":[{"output_type":"stream","name":"stdout","text":["전체 파라미터 수: 7272.78M\n","학습 가능한 파라미터 수: 0\n","파라미터 비율: 0.00%\n"]}],"source":["def print_trainable_params(model):\n","    trainable_params = 0\n","    all_params = 0\n","    for _, param in model.named_parameters():\n","        all_params += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(f\"전체 파라미터 수: {all_params / 1e6:.2f}M\")\n","    print(f\"학습 가능한 파라미터 수: {trainable_params}\")\n","    print(f\"파라미터 비율: {100 * trainable_params / all_params:.2f}%\")\n","\n","print_trainable_params(model)"]},{"cell_type":"markdown","metadata":{"id":"awj7t6jH_W57"},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"klSGHcInLzp4","outputId":"ad2a983a-0dcd-4b74-d035-8e31238f7905"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gg9Sf-C1jnMa","outputId":"4472022c-05a0-499b-bac7-6191b54aac01"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training set size: 3013, Validation set size: 200, Test set size: 200\n","Total training data point size: 0\n"]}],"source":["import os\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import random\n","\n","dataset_path = os.path.join('/content/drive/MyDrive/Inthon/content', 'dataset.csv')\n","data_df = pd.read_csv(dataset_path)\n","\n","train_df = data_df[data_df['train'] == True]\n","val_df = data_df[data_df['val'] == True]\n","test_df = data_df[data_df['test'] == True]\n","print(f\"Training set size: {len(train_df)}, Validation set size: {len(val_df)}, Test set size: {len(test_df)}\")\n","\n","num_epochs = 0\n","print(f\"Total training data point size: {len(train_df) * num_epochs}\")"]},{"cell_type":"markdown","metadata":{"id":"voO4tj3D_aDu"},"source":["## Define Custom Dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fgg3RNjeeUo-"},"outputs":[],"source":["from torch.utils.data import Dataset\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","\n","class CustomImageCaptionDataset(Dataset):\n","    def __init__(self, df, processor, test=False):\n","        self.df = df\n","        self.processor = processor\n","        self.test = test\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        if not self.test:\n","            image_url = self.df.iloc[idx][\"url\"]\n","            image_id = self.df.iloc[idx][\"Image_ID\"]\n","            caption = self.df.iloc[idx][\"Paragraph\"]\n","\n","            response = requests.get(image_url)\n","            image = Image.open(BytesIO(response.content)).convert('RGB')\n","\n","            inputs = self.processor(\n","                images=image,\n","                text=caption,\n","                padding=\"max_length\",\n","                truncation=True,\n","                max_length=256,\n","                return_tensors=\"pt\"\n","            )\n","            inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n","            inputs['labels'] = inputs['input_ids'].clone()\n","\n","            inputs['image_url'] = image_url\n","            inputs['Image_ID'] = image_id\n","            inputs['reference_caption'] = caption\n","\n","        else:\n","            image_url = self.df.iloc[idx][\"url\"]\n","            image_id = self.df.iloc[idx][\"Image_ID\"]\n","\n","            inputs = dict()\n","            inputs['image_url'] = image_url\n","            inputs['Image_ID'] = image_id\n","\n","        return inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aziATQWeglTv"},"outputs":[],"source":["train_dataset = CustomImageCaptionDataset(train_df, processor)\n","train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n","\n","val_dataset = CustomImageCaptionDataset(val_df, processor)\n","val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=8)\n","\n","test_dataset = CustomImageCaptionDataset(test_df, processor, test=True)\n","test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=8)"]},{"cell_type":"markdown","metadata":{"id":"1tuHBOkh_2iJ"},"source":["## Evaluation and Metric Calculation\n","We calculate SPICE, CLIPScore, and CHAIRf metrics on the test dataset. \\\\\n","\n","\n","**SPICE**: A score that evaluates text quality by measuring semantic content based on objects, relationships, and attributes present in image captions. \\\\\n","\n","**CLIPScore**: A metric that measures the similarity between images and text using the CLIP model. \\\\\n","\n","**CHAIR**: A metric that evaluates how well captions align with images, focusing on examining which objects are included in the captions. Unlike CHAIRi and CHAIRs which are more precision-oriented, CHAIRf considers both precision and recall aspects in its evaluation. \\\\\n","\n","\n","SPICE: https://arxiv.org/abs/1607.08822 \\\\\n","CLIPScore: https://arxiv.org/abs/2104.08718 \\\\\n","CHAIR: https://arxiv.org/abs/1809.02156 \\\\"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rrFXFyLgguSF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"81aa1af8-852d-436f-f5df-8f2f55cfed50"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["from transformers import CLIPProcessor, CLIPModel\n","import torch\n","from tqdm import tqdm\n","import requests\n","from PIL import Image\n","from io import BytesIO\n","from pycocoevalcap.spice.spice import Spice\n","from collections import Counter\n","import spacy\n","\n","evaluation_objects = [\n","    \"man\", \"woman\", \"tree\", \"sky\", \"building\", \"window\", \"shirt\", \"wall\",\n","    \"sign\", \"grass\", \"water\", \"table\", \"train\", \"plate\", \"car\", \"dog\", \"cat\",\n","    \"giraffe\", \"light\", \"pole\", \"plane\", \"boy\", \"zebra\", \"bus\", \"elephant\",\n","    \"ground\", \"hair\", \"girl\", \"horse\", \"cloud\", \"hand\", \"clock\", \"people\",\n","    \"snow\", \"bird\", \"chair\", \"fence\", \"glass\", \"floor\", \"bear\", \"boat\",\n","    \"street\", \"head\", \"door\", \"road\", \"shoe\", \"leg\", \"eye\", \"hat\"\n","]\n","spacy.prefer_gpu()\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n","clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","spice_scorer = Spice()\n","\n","def get_singular_form(word):\n","    return word.lemma_\n","\n","def extract_objects_from_caption(caption, object_list):\n","    doc = nlp(caption)\n","    objects_in_caption = set()\n","\n","    for token in doc:\n","        word = token.lemma_.lower()\n","        if word in object_list:\n","            objects_in_caption.add(word)\n","\n","    return objects_in_caption\n","\n","def calculate_chair_metrics(generated_caption, image_objects):\n","    caption_objects = extract_objects_from_caption(generated_caption, evaluation_objects)\n","\n","    hallucinated_objects = caption_objects - set(image_objects)\n","    missing_objects = set(image_objects) - caption_objects\n","    true_positives = caption_objects & set(image_objects)\n","\n","    precision = len(true_positives) / (len(true_positives) + len(hallucinated_objects)) if len(caption_objects) > 0 else 0\n","    recall = len(true_positives) / (len(true_positives) + len(missing_objects)) if len(image_objects) > 0 else 0\n","\n","    chairf = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","\n","    print(f\"환각 객체: {hallucinated_objects}, 누락 객체: {missing_objects}, 캡션 객체: {caption_objects}, 이미지 객체: {image_objects}\")\n","\n","    return chairf\n","\n","def calculate_metrics(image, generated_caption, reference_caption, image_objects):\n","    spice_score, _ = spice_scorer.compute_score({0: [reference_caption]}, {0: [generated_caption]})\n","\n","    inputs = clip_processor(text=generated_caption,\n","                            images=image,\n","                            return_tensors=\"pt\",\n","                            padding=\"max_length\",\n","                            truncation=True,\n","                            max_length=77).to(device)\n","    outputs = clip_model(**inputs)\n","    logits_per_image = outputs.logits_per_image\n","    clip_score = logits_per_image.item()\n","\n","    chairf = calculate_chair_metrics(generated_caption, image_objects)\n","    print(f'spice_score: {spice_score}, clip_score: {clip_score}, chairf: {chairf}')\n","\n","    return spice_score, clip_score, chairf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jTODC3vMuRkn","colab":{"base_uri":"https://localhost:8080/","height":389},"outputId":"5b220d1b-6492-41fe-94e6-3e584313a17b"},"outputs":[{"output_type":"stream","name":"stderr","text":["Calculating SPICE, CLIPScore, and CHAIRf:   0%|          | 0/25 [00:04<?, ?it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-4a87c610be82>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Calculating SPICE, CLIPScore, and CHAIRf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mimage_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mimage_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Image_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-fbb53a76beb8>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Paragraph\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","val_results = []\n","\n","with torch.no_grad():\n","    for batch in tqdm(val_dataloader, desc=\"Calculating SPICE, CLIPScore, and CHAIRf\"):\n","        image_urls = batch['image_url']\n","        image_ids = batch['Image_ID']\n","        reference_captions = batch['reference_caption']\n","\n","        images = []\n","        for image_url in image_urls:\n","            response = requests.get(image_url)\n","            image = Image.open(BytesIO(response.content)).convert('RGB')\n","            images.append(image)\n","\n","        image_objects_batch = [\n","            extract_objects_from_caption(ref_caption, evaluation_objects)\n","            for ref_caption in reference_captions\n","        ]\n","\n","        inputs = processor(images=images, text=['Describe the background of the image, describe every object in the image and its appearance in detail.'] * len(images), return_tensors=\"pt\").to(device)\n","\n","        generated_ids = model.generate(\n","            **inputs,\n","            do_sample=True,\n","            num_beams=4,\n","            max_length=160,\n","            min_length=100,\n","            top_p=0.92,\n","            repetition_penalty=1.2,\n","            length_penalty=1.1,\n","            temperature=0.7\n","        )\n","\n","        generated_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n","        generated_captions = [caption.strip() for caption in generated_captions]\n","\n","        print(generated_captions)\n","\n","        existing_obj = [[] for _ in range(len(generated_captions))]\n","\n","        for i, caption in enumerate(generated_captions):\n","            for obj in evaluation_objects:\n","                if ' ' + obj + ' ' in caption.lower() or ' ' + obj + '.' in caption.lower() or ' ' + obj + ',' in caption.lower():\n","                    existing_obj[i].append(obj)\n","\n","\n","        prompts = [\"Simply describe following objects in the image: \" + \", \".join(obj_list) + \". Focus on their appearance, color, actions, and interactions with surrounding objects.\"\n","                    for obj_list in existing_obj]\n","\n","        print(existing_obj)\n","\n","        inputs = processor(images=images, text=prompts, return_tensors=\"pt\", padding=True).to(device)\n","\n","        generated_ids = model.generate(\n","            **inputs,\n","            do_sample=True,\n","            num_beams=4,\n","            max_length=180,\n","            min_length=120,\n","            top_p=0.92,\n","            repetition_penalty=1.2,\n","            length_penalty=1.1,\n","            temperature=0.7\n","        )\n","\n","        generated_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n","        generated_captions = [caption.strip() for caption in generated_captions]\n","\n","\n","        for image_id, generated_caption, reference_caption, image, image_objects in zip(\n","            image_ids, generated_captions, reference_captions, images, image_objects_batch\n","        ):\n","            spice_score, clip_score, chairf = calculate_metrics(\n","                image, generated_caption, reference_caption, image_objects\n","            )\n","\n","            plt.imshow(image)\n","            plt.axis('off')\n","            plt.show()\n","\n","            print(f\"Generated Caption: {generated_caption}\")\n","            print(f\"Reference Caption: {reference_caption}\")\n","\n","            val_results.append({\n","                \"Image_ID\": image_id,\n","                \"generated_caption\": generated_caption,\n","                \"reference_caption\": reference_caption,\n","                \"spice_score\": spice_score,\n","                \"clip_score\": clip_score,\n","                \"chairf\": chairf,\n","            })\n","\n","\n","        results_df = pd.DataFrame(val_results)\n","\n","        average_spice = results_df[\"spice_score\"].mean()\n","        average_clip_score = results_df[\"clip_score\"].mean()\n","        average_chairf = results_df[\"chairf\"].mean()\n","\n","        print(f\"Average SPICE Score: {average_spice:.4f}\")\n","        print(f\"Average CLIPScore: {average_clip_score:.4f}\")\n","        print(f\"Average CHAIRf Score: {average_chairf:.4f}\")\n","\n","        def calculate_custom_score(spice_score, clip_score, chairf):\n","            custom_score = (0.4 * spice_score) + (0.2 * (clip_score / 250)) + (0.4 * chairf)\n","            return custom_score\n","\n","        results_df[\"custom_score\"] = results_df.apply(\n","            lambda row: calculate_custom_score(\n","                    row[\"spice_score\"], row[\"clip_score\"], row[\"chairf\"]\n","            ), axis=1\n","        )\n","\n","        average_custom_score = results_df[\"custom_score\"].mean()\n","        print(f\"Average Custom Score: {average_custom_score:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_Xwr5PP6Rfp"},"outputs":[],"source":["results_df = pd.DataFrame(val_results)\n","\n","average_spice = results_df[\"spice_score\"].mean()\n","average_clip_score = results_df[\"clip_score\"].mean()\n","average_chairf = results_df[\"chairf\"].mean()\n","\n","print(f\"Average SPICE Score: {average_spice:.4f}\")\n","print(f\"Average CLIPScore: {average_clip_score:.4f}\")\n","print(f\"Average CHAIRf Score: {average_chairf:.4f}\")\n","\n","def calculate_custom_score(spice_score, clip_score, chairf):\n","    custom_score = (0.4 * spice_score) + (0.2 * (clip_score / 250)) + (0.4 * chairf)\n","    return custom_score\n","\n","results_df[\"custom_score\"] = results_df.apply(\n","    lambda row: calculate_custom_score(\n","        row[\"spice_score\"], row[\"clip_score\"], row[\"chairf\"]\n","    ), axis=1\n",")\n","\n","average_custom_score = results_df[\"custom_score\"].mean()\n","print(f\"Average Custom Score: {average_custom_score:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"78IFiqtTACTx"},"source":["## Save Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-xnmvaSuUqU","colab":{"base_uri":"https://localhost:8080/","height":482},"outputId":"0986877d-1db7-4654-9b31-61e7c67faa9c"},"outputs":[{"output_type":"stream","name":"stderr","text":["\rGenerating Captions:   0%|          | 0/25 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["['In the background of the image, there is a train station with a red train on the tracks. The train appears to be stopped at the station, possibly waiting for passengers to board or disembark. The train station is surrounded by trees and other greenery, providing a scenic backdrop for the train. Additionally, there are several people standing around the train station, likely waiting for their train or observing the scene.', 'In the image, a man is standing in front of a brick fireplace. He is holding a video game controller and appears to be playing a bowling video game on a TV mounted above the fireplace. The TV is positioned high up on the wall, making it easy for the man to see the game while standing in front of the fireplace. There are also a few books scattered around the room, adding to the cozy atmosphere of the scene.', 'In the background of the image, there is a green building with a sign that reads \"Irish Independent Delicatessen.\" There is also a traffic light in front of the building, which is visible in the foreground of the image. The traffic light is positioned to the right of the green building, making it a prominent feature in the scene. Additionally, there are several cars parked in front of the building, contributing to the overall composition of the image.', 'In the background of the image, there is a sailboat floating in a body of water. The sailboat is accompanied by a few trees, which can be seen in the foreground and background of the image. The sailboat appears to be sailing towards the left side of the image, with its mast and sails visible. There are also a few other boats visible in the image, but they are not as prominent as the main sailboat. Overall, the image captures a peaceful and serene scene of a sailboat on the water, surrounded by trees and other boats.', \"In the background of the image, there is a large bear walking through a grassy field. The bear is walking towards the left side of the image, making its way through the lush greenery. There are also a few trees scattered throughout the scene, adding to the natural setting of the image. The bear's presence in the grassy field creates a sense of serenity and peacefulness, as if it's just taking a leisurely stroll through the wilderness.\", \"In the background of the image, there is a train station with a green and yellow train parked on the tracks. The train station is surrounded by tall buildings, giving the impression of a bustling urban environment. The train appears to be traveling along the tracks, possibly on its way to another destination. There is also a pedestrian bridge in the scene, likely connecting the train station to other parts of the city. Additionally, there is a person walking on the sidewalk near the train station, adding to the scene's sense of activity and movement.\", 'The image features a group of people riding horses on a rocky terrain. There are three people riding horses, with one person in the foreground and the other two in the background. The rocky terrain provides a challenging and adventurous environment for the horseback riding. The people appear to be enjoying the scenic view and the experience of riding through the mountains.', 'In the background of the image, there is a cat sitting on a desk. The cat is black and appears to be relaxing on the desk. There is also a laptop on the desk, suggesting that the cat may be using the computer or simply enjoying its presence on the desk. Additionally, there are several other items on the desk, including a mouse, a stack of papers, and a phone. The overall scene appears to be a cozy and comfortable workspace with the cat adding a touch of warmth and companionship.']\n","[['train', 'people'], ['man', 'wall'], ['building', 'sign', 'light'], ['water'], ['bear'], ['train'], ['people'], ['cat']]\n"]},{"output_type":"stream","name":"stderr","text":["\rGenerating Captions:   4%|▍         | 1/25 [01:03<25:16, 63.20s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","['a red train and a group of people standing on the train platform, waiting for the train to arrive. The train is in the process of pulling into the station, with people standing on the platform waiting for the train to arrive. The train is a red color, and the people are standing on the platform, waiting for the train to arrive. The train is a red color, and the people are standing on the platform, waiting for the train to arrive. The train is a red color, and the people are standing on the platform, waiting for the train to arrive. The train is a red color, and the people are standing on the platform, waiting for the train to arrive. The train is a red color, and the', \"A man in a blue shirt is standing in front of a brick wall, holding a white bowling ball. The man is playing a game of bowling on a large screen television, which is mounted on the wall in front of him. The television is displaying the game of bowling, and the man is holding the bowling ball as if he is about to throw it down the lane. The man's actions suggest that he is actively participating in the game of bowling on the television screen.\", 'a green building with a sign on it and a traffic light in front of the building\\na traffic light in front of a green building with a sign on it, and a sandwich board in front of the building\\nthe traffic light is in front of a green building with a sign that says \"Irish Independent Delicatessen\" and a sandwich board in front of the building, advertising the delicatessen\\'s sandwiches', 'a sailboat in the water\\n* a sailboat in the water\\n* a sailboat in the water\\n* a sailboat in the water\\n* a sailboat in the water\\n* a sailboat in the water\\n* a sailboat in the water\\n* a sailboat in the water\\n* a sailboat in the water\\n* a sailboat in the water\\n* a sailboat in the water\\n* a sailboat in the water\\n* a sailboat in the water\\n* a sailboat in the water\\n* a sailboat in the water\\n* a sailboat in the water\\n* a sailbo', \"In the image, there is a large black bear walking through a grassy field. The bear is walking towards the left side of the image, making its way through the lush green grass. The bear appears to be on the move, possibly searching for food or exploring its surroundings. The bear's presence in the grassy field suggests that it is in a natural environment, such as a forest or a wilderness area.\", \"a green and yellow train is traveling down the train tracks, passing through a city with tall buildings in the background. The train appears to be on its way to a station, possibly a train station, as it moves along the tracks. The train's color combination of green and yellow creates an eye-catching appearance, making it stand out among the other objects in the scene. Additionally, the train's presence highlights the transportation aspect of the image, emphasizing the movement of people and goods from one place to another.\", 'In the image, there are two people riding horses up a rocky hill. One of the people is wearing a white shirt, while the other person is wearing a black shirt. The people are riding their horses up the rocky hill together, creating a sense of camaraderie and adventure. The mountainous terrain provides a scenic backdrop for the horseback riding experience, adding to the excitement and thrill of the adventure.', \"a black cat sitting on top of a wooden desk in front of a laptop computer. The cat is sitting on top of the desk in front of a laptop computer, which is open and visible in the image. The cat appears to be resting on the desk, possibly waiting for someone to interact with it or simply enjoying its presence in the room. The cat's color is black, making it stand out against the wooden background of the desk.\"]\n"]},{"output_type":"stream","name":"stderr","text":["\rGenerating Captions:   4%|▍         | 1/25 [01:08<27:29, 68.73s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-04f840daed73>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage_url\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_urls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["submission_data = []\n","\n","with torch.no_grad():\n","    for batch in tqdm(test_dataloader, desc=\"Generating Captions\"):\n","        image_urls = batch['image_url']\n","        image_ids = batch['Image_ID']\n","\n","        images = []\n","        for image_url in image_urls:\n","            response = requests.get(image_url)\n","            image = Image.open(BytesIO(response.content)).convert('RGB')\n","            images.append(image)\n","\n","        inputs = processor(images=images, text=['Describe the background of the image, describe every object in the image and its appearance in detail.'] * len(images), return_tensors=\"pt\").to(device)\n","        generated_ids = model.generate(\n","            **inputs,\n","            do_sample=True,\n","            num_beams=4,\n","            max_length=160,\n","            min_length=100,\n","            top_p=0.92,\n","            repetition_penalty=1.2,\n","            length_penalty=1.1,\n","            temperature=0.7\n","        )\n","\n","        generated_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n","        generated_captions = [caption.strip() for caption in generated_captions]\n","\n","        print(generated_captions)\n","\n","        existing_obj = [[] for _ in range(len(generated_captions))]\n","\n","        for i, caption in enumerate(generated_captions):\n","            for obj in evaluation_objects:\n","                if ' ' + obj + ' ' in caption.lower() or ' ' + obj + '.' in caption.lower() or ' ' + obj + ',' in caption.lower():\n","                    existing_obj[i].append(obj)\n","\n","        print(existing_obj)\n","\n","        prompts = [\"Simply describe following objects in the image: \" + \", \".join(obj_list) + \". Focus on their appearance, color, actions, and interactions with surrounding objects.\"\n","                    for obj_list in existing_obj]\n","\n","        inputs = processor(images=images, text=prompts, return_tensors=\"pt\", padding=True).to(device)\n","        generated_ids = model.generate(\n","            **inputs,\n","            do_sample=True,\n","            num_beams=4,\n","            max_length=180,\n","            min_length=120,\n","            top_p=0.92,\n","            repetition_penalty=1.2,\n","            length_penalty=1.1,\n","            temperature=0.7\n","        )\n","\n","        generated_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n","        generated_captions = [caption.strip() for caption in generated_captions]\n","\n","        print()\n","        print(generated_captions)\n","\n","        for image_id, generated_caption in zip(\n","            image_ids, generated_captions\n","        ):\n","            submission_data.append({\n","                \"Image_ID\": image_id,\n","                \"generated_caption\": generated_caption\n","            })\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ruhRNzz8kWDX"},"outputs":[],"source":["submission_df = pd.DataFrame(submission_data)\n","submission_df.to_csv(\"submission_team4_13.csv\", index=False)\n","\n","print(\"Submission file 'submission.csv' created successfully.\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"976979ba6bdc41be800b5807fe46b9f3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5731b978f49b4d529d45c7d4ec1c4037","IPY_MODEL_e0fff08c5eb94cb5a6e14a883a97548c","IPY_MODEL_eb35675874ec4563b595c117cba67968"],"layout":"IPY_MODEL_f571337c48a54c3fa0b2fcbb6aef909f"}},"5731b978f49b4d529d45c7d4ec1c4037":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f01d59ce97464ead8774915ece792b2c","placeholder":"​","style":"IPY_MODEL_314c838c841e44b6b0a5c5da93c8a2c9","value":"Loading checkpoint shards: 100%"}},"e0fff08c5eb94cb5a6e14a883a97548c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_31aeb5284e5a4ffa9478c8fdf350eefe","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_319391f887bd455d8e07d31c27b6d97c","value":6}},"eb35675874ec4563b595c117cba67968":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f14caabd7d6481f9100ccdb6455724e","placeholder":"​","style":"IPY_MODEL_2611ab9861b9491c9635d0cbac11d10b","value":" 6/6 [04:44&lt;00:00, 45.21s/it]"}},"f571337c48a54c3fa0b2fcbb6aef909f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f01d59ce97464ead8774915ece792b2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"314c838c841e44b6b0a5c5da93c8a2c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31aeb5284e5a4ffa9478c8fdf350eefe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"319391f887bd455d8e07d31c27b6d97c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0f14caabd7d6481f9100ccdb6455724e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2611ab9861b9491c9635d0cbac11d10b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}